{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "from energyemissionsregio.config import DATA_PATH\n",
    "from energyemissionsregio.utils import get_confidence_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_df = pd.read_excel(\n",
    "    os.path.join(DATA_PATH, \"..\", \"..\", \"01_raw\", \"variables_with_details_and_tags.xlsx\"),\n",
    "    sheet_name=\"collected_variables_EU\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_vars_df(predictor_vars):\n",
    "    X_vars_df = None\n",
    "\n",
    "    for var_name in predictor_vars:\n",
    "        spatial_resolution = var_df[var_df[\"var_name\"] == var_name][\n",
    "                    \"spatial_level\"\n",
    "                ].values[0]\n",
    "\n",
    "        if spatial_resolution == \"LAU\":\n",
    "        \n",
    "            _df = pd.read_csv(\n",
    "                os.path.join(DATA_PATH, f\"{var_name}.csv\")\n",
    "            )\n",
    "            _df = _df[_df[\"region_code\"].str.startswith((\"DE\", \"ES\"))][[\"region_code\", \"value\"]].copy()\n",
    "\n",
    "            _df = _df.fillna(0) # filling NAs for point vars. Non-point vars have no NAs in Germany\n",
    "\n",
    "            #convert LAU to NUTS3 regions\n",
    "            _df[\"region_code\"] = _df[\"region_code\"].str.split(\"_\").str[0]\n",
    "\n",
    "            # aggregate per NUTS3 region \n",
    "            agg_method = var_df[var_df[\"var_name\"] == var_name][\n",
    "                    \"var_aggregation_method\"\n",
    "                ].values[0]\n",
    "\n",
    "            if agg_method == \"SUM\":\n",
    "                _df = _df.groupby(\"region_code\").sum().reset_index()\n",
    "            elif agg_method == \"AVG\":\n",
    "                _df = _df.groupby(\"region_code\").mean().reset_index()\n",
    "            elif agg_method == \"MAX\":\n",
    "                _df = _df.groupby(\"region_code\").max().reset_index()\n",
    "            else:\n",
    "                raise ValueError(\"Unknown var aggregation method\")\n",
    "\n",
    "            _df.rename(columns={\"value\": var_name}, inplace=True)\n",
    "\n",
    "            if X_vars_df is not None:\n",
    "                X_vars_df = pd.merge(X_vars_df, _df, on=\"region_code\", how=\"outer\")\n",
    "            else:\n",
    "                X_vars_df = _df\n",
    "\n",
    "        else:\n",
    "            if var_name.startswith(\"cproj_\"):\n",
    "\n",
    "                _df_de = pd.read_csv(os.path.join(DATA_PATH, \"..\", \"climate_projections\", \"DE\", var_name, \"2020.csv\"))\n",
    "                _df_de = _df_de[_df_de[\"climate_experiment\"] == \"RCP4.5\"].copy()\n",
    "\n",
    "                _df_de.drop(columns=\"climate_experiment\", inplace=True)\n",
    "\n",
    "                _df_es = pd.read_csv(os.path.join(DATA_PATH, \"..\", \"climate_projections\", \"ES\", var_name, \"2020.csv\"))\n",
    "                _df_es = _df_es[_df_es[\"climate_experiment\"] == \"RCP4.5\"].copy()\n",
    "\n",
    "                _df_es.drop(columns=\"climate_experiment\", inplace=True)\n",
    "\n",
    "                _df = pd.concat([_df_de, _df_es])\n",
    "\n",
    "            else:\n",
    "                _df = pd.read_csv(\n",
    "                    os.path.join(DATA_PATH, f\"{var_name}.csv\")\n",
    "                )\n",
    "                _df = _df[_df[\"region_code\"].str.startswith((\"ES\", \"DE\"))][[\"region_code\", \"value\"]].copy()\n",
    "            \n",
    "            \n",
    "            _df.rename(columns={\"value\": var_name}, inplace=True)\n",
    "\n",
    "            if X_vars_df is not None:\n",
    "                X_vars_df = pd.merge(X_vars_df, _df, on=\"region_code\", how=\"outer\")\n",
    "            else:\n",
    "                X_vars_df = _df\n",
    "\n",
    "    return X_vars_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_details = [\n",
    "    {\"var_to_impute\": \"es_number_of_commerical_and_service_companies\",\n",
    "                    \"best_corr_threshold\": 0.1, \n",
    "                    \"r2\": 0.61},\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vars_detail_dict in vars_details:\n",
    "    var_to_impute = vars_detail_dict[\"var_to_impute\"]\n",
    "    best_corr_threshold = vars_detail_dict[\"best_corr_threshold\"]\n",
    "    r2 = vars_detail_dict[\"r2\"]\n",
    "\n",
    "    imputed_value_confidence_level = get_confidence_level(r2)\n",
    "\n",
    "    print(var_to_impute)\n",
    "\n",
    "    with open(\n",
    "            os.path.join(\n",
    "                cwd, \"..\", \"..\", \"data\", \n",
    "                \"missing_value_imputation\",\n",
    "                  \"predictor_vars\", \n",
    "                  f\"{var_to_impute}_{best_corr_threshold}corr.json\"\n",
    "            )\n",
    "        ) as f:\n",
    "            predictor_vars = tuple(json.load(f))\n",
    "\n",
    "    X_vars_df = get_x_vars_df(predictor_vars)\n",
    "\n",
    "    y_var_df = pd.read_csv(\n",
    "        os.path.join(DATA_PATH, f\"{var_to_impute}.csv\")\n",
    "    )\n",
    "\n",
    "    y_var_df.rename(columns={\"value\": var_to_impute}, inplace=True)\n",
    "    y_var_df.drop(columns=\"year\", inplace=True)\n",
    "\n",
    "    final_df = pd.merge(X_vars_df, y_var_df, on=\"region_code\", how=\"left\")\n",
    "\n",
    "    # fit the model on missing data\n",
    "    missing_values_df = final_df.copy()\n",
    "    missing_values_df = missing_values_df[missing_values_df[var_to_impute].isna()].drop(columns=[var_to_impute])\n",
    "\n",
    "    missing_values_df_no_reg_code = missing_values_df.drop(columns=[\"region_code\"])\n",
    "\n",
    "    # Construct the file path\n",
    "    file_path = os.path.join(cwd, \"..\", \"..\", \"data\", \n",
    "                            \"missing_value_imputation\", \n",
    "                            \"models\", \n",
    "                            f\"{var_to_impute}_xgb_{best_corr_threshold}corr.pkl\")\n",
    "    \n",
    "    # Load the model from the pickle file\n",
    "    with open(file_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    " \n",
    "    y_pred = model.predict(missing_values_df_no_reg_code)\n",
    "\n",
    "    # round off values \n",
    "    if var_to_impute.startswith((\"de_employment\", \"de_number_of_passenger_cars\")):\n",
    "        y_pred = y_pred.astype(int)\n",
    "    else:\n",
    "         y_pred = y_pred.round(2)\n",
    "\n",
    "    # round off values (if int type is desired)\n",
    "    if var_to_impute == \"es_number_of_commerical_and_service_companies\":\n",
    "        y_pred = y_pred.astype(int)\n",
    "    else:\n",
    "        y_pred = y_pred.round(2)\n",
    "\n",
    "    missing_values_df['imputed_values'] = y_pred\n",
    "\n",
    "    assert missing_values_df[\"imputed_values\"].min() >= 0\n",
    "\n",
    "    # filling missing data and assign value_confidence_level\n",
    "\n",
    "    imputed_df = final_df[[\"region_code\", var_to_impute]].copy()\n",
    "    imputed_df.rename(columns={var_to_impute: \"value\"}, inplace=True) \n",
    "\n",
    "    imputed_df[\"value_confidence_level\"] = 5 # VERY HIGH \n",
    "\n",
    "    for idx, row in final_df.iterrows():\n",
    "        region_code = row[\"region_code\"]\n",
    "\n",
    "        if math.isnan(row[var_to_impute]):\n",
    "            impute_value = missing_values_df[missing_values_df[\"region_code\"] == region_code][[\"imputed_values\"]].values.item()\n",
    "\n",
    "            if \"number\" in var_to_impute:\n",
    "                imputed_df.loc[idx, \"value\"] = int(impute_value)\n",
    "            else:\n",
    "                imputed_df.loc[idx, \"value\"] = impute_value\n",
    "                 \n",
    "            imputed_df.loc[idx, \"value_confidence_level\"] = imputed_value_confidence_level\n",
    "\n",
    "    imputed_df.to_csv(os.path.join(\n",
    "            cwd, \"..\", \"..\", \"data\", \"imputed_data\", f\"{var_to_impute}.csv\"\n",
    "        ), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average daily traffic - light duty vehicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_to_impute = \"es_average_daily_traffic_light_duty_vehicles\"\n",
    "\n",
    "imputed_df = pd.read_csv(os.path.join(DATA_PATH, f\"{var_to_impute}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_df.drop(columns=\"year\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_df['value_confidence_level'] = imputed_df['value'].apply(lambda x: 5 if pd.notna(x) else 2) # VERY LOW if missing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_value = imputed_df['value'].mean()\n",
    "\n",
    "imputed_df['value'] = imputed_df['value'].fillna(mean_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_df.to_csv(os.path.join(\n",
    "            cwd, \"..\", \"..\", \"data\", \"imputed_data\", f\"{var_to_impute}.csv\"\n",
    "        ), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
