{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once the best model is chosen based on the experiments on the computational cluster, the model is used to impute the missing data here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "from energyemissionsregio.config import DATA_PATH\n",
    "from energyemissionsregio.utils import get_confidence_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_details = [\n",
    "    {\"var_to_impute\": \"es_utilized_agricultural_area\",\n",
    "                    \"best_corr_threshold\": 0.1, \n",
    "                    \"r2\": 0.89},\n",
    "\n",
    "                {\"var_to_impute\": \"es_number_of_dwellings\",\n",
    "                    \"best_corr_threshold\": 0.1, \n",
    "                    \"r2\":  0.97},\n",
    "                ]\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_vars_df(predictor_vars):\n",
    "    X_vars_df = None\n",
    "\n",
    "    for var_name in predictor_vars:\n",
    "        \n",
    "        _df = pd.read_csv(\n",
    "            os.path.join(DATA_PATH, f\"{var_name}.csv\")\n",
    "        )\n",
    "        _df = _df[_df[\"region_code\"].str.startswith((\"ES\", \"DE\"))][[\"region_code\", \"value\"]].copy()\n",
    "\n",
    "        _df = _df.fillna(0) # filling NAs for point vars. Non-point vars have no NAs in Spain\n",
    "\n",
    "        _df.rename(columns={\"value\": var_name}, inplace=True)\n",
    "\n",
    "        if X_vars_df is not None:\n",
    "            X_vars_df = pd.merge(X_vars_df, _df, on=\"region_code\", how=\"outer\")\n",
    "        else:\n",
    "            X_vars_df = _df\n",
    "\n",
    "    return X_vars_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vars_detail_dict in vars_details:\n",
    "    var_to_impute = vars_detail_dict[\"var_to_impute\"]\n",
    "    best_corr_threshold = vars_detail_dict[\"best_corr_threshold\"]\n",
    "    r2 = vars_detail_dict[\"r2\"]\n",
    "\n",
    "    imputed_value_confidence_level = get_confidence_level(r2)\n",
    "\n",
    "    print(var_to_impute)\n",
    "\n",
    "    with open(\n",
    "            os.path.join(\n",
    "                cwd, \"..\", \"..\", \"data\", \n",
    "                \"missing_value_imputation\",\n",
    "                  \"predictor_vars\", \n",
    "                  f\"{var_to_impute}_{best_corr_threshold}corr.json\"\n",
    "            )\n",
    "        ) as f:\n",
    "            predictor_vars = tuple(json.load(f))\n",
    "\n",
    "    X_vars_df = get_x_vars_df(predictor_vars)\n",
    "\n",
    "    y_var_df = pd.read_csv(\n",
    "        os.path.join(DATA_PATH, f\"{var_to_impute}.csv\")\n",
    "    )\n",
    "\n",
    "    y_var_df.rename(columns={\"value\": var_to_impute}, inplace=True)\n",
    "    y_var_df.drop(columns=\"year\", inplace=True)\n",
    "\n",
    "    final_df = pd.merge(X_vars_df, y_var_df, on=\"region_code\", how=\"left\")\n",
    "\n",
    "    # fit the model on missing data\n",
    "    missing_values_df = final_df.copy()\n",
    "    missing_values_df = missing_values_df[missing_values_df[\n",
    "                            var_to_impute].isna()].drop(columns=[var_to_impute])\n",
    "\n",
    "    missing_values_df_no_reg_code = missing_values_df.drop(columns=[\"region_code\"])\n",
    "\n",
    "    file_path = os.path.join(cwd, \"..\", \"..\", \"data\", \n",
    "                            \"missing_value_imputation\", \n",
    "                            \"models\", \n",
    "                            f\"{var_to_impute}_xgb_{best_corr_threshold}corr.pkl\")\n",
    "    \n",
    "    with open(file_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    " \n",
    "    y_pred = model.predict(missing_values_df_no_reg_code)\n",
    "\n",
    "    missing_values_df['imputed_values'] = y_pred\n",
    "\n",
    "    assert missing_values_df[\"imputed_values\"].min() >= 0\n",
    "\n",
    "    # filling missing data and assign value_confidence_level\n",
    "    imputed_df = final_df[[\"region_code\", var_to_impute]].copy()\n",
    "    imputed_df.rename(columns={var_to_impute: \"value\"}, inplace=True) \n",
    "\n",
    "    imputed_df[\"value_confidence_level\"] = 5 # VERY HIGH \n",
    "\n",
    "    for idx, row in final_df.iterrows():\n",
    "        region_code = row[\"region_code\"]\n",
    "\n",
    "        if math.isnan(row[var_to_impute]):\n",
    "            impute_value = missing_values_df[missing_values_df[\n",
    "                                \"region_code\"] == region_code][[\"imputed_values\"]].values.item()\n",
    "            \n",
    "            # round off values (if int type is desired)\n",
    "            if \"number\" in var_to_impute:\n",
    "                imputed_df.loc[idx, \"value\"] = int(impute_value)\n",
    "            else:\n",
    "                imputed_df.loc[idx, \"value\"] = impute_value\n",
    "\n",
    "            imputed_df.loc[idx, \"value_confidence_level\"] = imputed_value_confidence_level\n",
    "\n",
    "    imputed_df.to_csv(os.path.join(\n",
    "            cwd, \"..\", \"..\", \"data\", \"imputed_data\", f\"{var_to_impute}.csv\"\n",
    "        ), index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
