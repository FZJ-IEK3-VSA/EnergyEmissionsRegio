{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from energyemissionsregio.config import DATA_PATH\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### potential predictor variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lau_vars = [\n",
    "    \"population\", \n",
    "             \"area\", \n",
    "             'continuous_urban_fabric_cover',\n",
    "       'discontinuous_urban_fabric_cover',\n",
    "       'industrial_or_commercial_units_cover',\n",
    "       'port_areas_cover',\n",
    "       'airports_cover', 'mineral_extraction_sites_cover',\n",
    "       'dump_sites_cover', 'construction_sites_cover',\n",
    "       'green_urban_areas_cover', 'sport_and_leisure_facilities_cover',\n",
    "       'non_irrigated_arable_land_cover',\n",
    "       'permanently_irrigated_land_cover', 'rice_fields_cover',\n",
    "       'vineyards_cover', 'fruit_trees_and_berry_plantations_cover',\n",
    "       'olive_groves_cover', 'pastures_cover', 'permanent_crops_cover',\n",
    "       'complex_cultivation_patterns_cover',\n",
    "       'agriculture_with_natural_vegetation_cover',\n",
    "       'agro_forestry_areas_cover', 'broad_leaved_forest_cover',\n",
    "       'coniferous_forest_cover', 'mixed_forest_cover',\n",
    "       'natural_grasslands_cover', 'moors_and_heathland_cover',\n",
    "       'sclerophyllous_vegetation_cover',\n",
    "       'transitional_woodland_shrub_cover',\n",
    "       'beaches_dunes_and_sand_cover', \n",
    "       'bare_rocks_cover',\n",
    "       'sparsely_vegetated_areas_cover', 'burnt_areas_cover',\n",
    "       'glaciers_and_perpetual_snow_cover', 'inland_marshes_cover',\n",
    "       'peat_bogs_cover', \n",
    "       'salt_marshes_cover', \n",
    "       'salines_cover',\n",
    "       'intertidal_flats_cover', 'water_courses_cover',\n",
    "       'water_bodies_cover', 'coastal_lagoons_cover', 'estuaries_cover',\n",
    "       'sea_and_ocean_cover',\n",
    "       \"number_of_iron_and_steel_industries\",\n",
    "      \"number_of_cement_industries\",\n",
    "      \"number_of_refineries\",\n",
    "      \"number_of_paper_and_printing_industries\",\n",
    "      \"number_of_chemical_industries\",\n",
    "      \"number_of_glass_industries\",\n",
    "      \"number_of_non_ferrous_metals_industries\",\n",
    "      \"number_of_non_metallic_minerals_industries\",\n",
    "       \"railway_network\",\n",
    "       \"road_network\",\n",
    "       \"number_of_buildings\",\n",
    "         'average_air_pollution_due_to_pm25',\n",
    "       'average_air_pollution_due_to_no2',\n",
    "       'average_air_pollution_due_to_o3',\n",
    "       'average_air_pollution_due_to_pm10'\n",
    "             ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUTS3 data with no missing values\n",
    "nuts3_vars = [\"employment_in_agriculture_forestry_and_fishing\", \n",
    "                  \"employment_in_manufacturing\",\n",
    "                  \"employment_in_construction\",\n",
    "                  \"gross_domestic_product\",\n",
    "                  \"road_transport_of_freight\",\n",
    "                  \"soil_sealing\",\n",
    "                    'number_of_buffaloes', 'number_of_cattle', 'number_of_pigs',\n",
    "       'number_of_sheeps', 'number_of_chickens', 'number_of_goats',\n",
    "                  \"cproj_annual_mean_temperature_heating_degree_days\",\n",
    "                  \"cproj_annual_mean_temperature_cooling_degree_days\",\n",
    "                       \n",
    "             ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_df = pd.read_excel(\n",
    "    os.path.join(DATA_PATH, \"..\", \"..\", \"01_raw\", \"variables_with_details_and_tags.xlsx\"),\n",
    "    sheet_name=\"collected_variables_EU\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vars_lau = lau_vars.copy()\n",
    "x_vars_nuts3 = nuts3_vars.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vars_df = None\n",
    "\n",
    "for var_name in x_vars_lau:\n",
    "    \n",
    "    _df = pd.read_csv(\n",
    "        os.path.join(DATA_PATH, f\"{var_name}.csv\")\n",
    "    )\n",
    "    _df = _df[_df[\"region_code\"].str.startswith(\"ES\")][[\"region_code\", \"value\"]].copy()\n",
    "\n",
    "    _df = _df.fillna(0) # filling NAs for point vars. Non-point vars have no NAs in Germany\n",
    "\n",
    "    #convert LAU to NUTS3 regions\n",
    "    _df[\"region_code\"] = _df[\"region_code\"].str.split(\"_\").str[0]\n",
    "\n",
    "    # aggregate per NUTS3 region \n",
    "    agg_method = var_df[var_df[\"var_name\"] == var_name][\n",
    "            \"var_aggregation_method\"\n",
    "        ].values[0]\n",
    "\n",
    "    if agg_method == \"SUM\":\n",
    "        _df = _df.groupby(\"region_code\").sum().reset_index()\n",
    "    elif agg_method == \"AVG\":\n",
    "        _df = _df.groupby(\"region_code\").mean().reset_index()\n",
    "    elif agg_method == \"MAX\":\n",
    "        _df = _df.groupby(\"region_code\").max().reset_index()\n",
    "    else:\n",
    "        raise ValueError(\"Unknown var aggregation method\")\n",
    "\n",
    "    _df.rename(columns={\"value\": var_name}, inplace=True)\n",
    "\n",
    "    if X_vars_df is not None:\n",
    "        X_vars_df = pd.merge(X_vars_df, _df, on=\"region_code\", how=\"outer\")\n",
    "    else:\n",
    "        X_vars_df = _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_name in x_vars_nuts3:\n",
    "    if var_name.startswith(\"cproj_\"):\n",
    "        _df = pd.read_csv(os.path.join(DATA_PATH, \"..\", \"climate_projections\", \"ES\", var_name, \"2020.csv\"))\n",
    "        _df = _df[_df[\"climate_experiment\"] == \"RCP4.5\"].copy()\n",
    "\n",
    "        _df.drop(columns=\"climate_experiment\", inplace=True)\n",
    "\n",
    "    else:\n",
    "        _df = pd.read_csv(\n",
    "            os.path.join(DATA_PATH, f\"{var_name}.csv\")\n",
    "        )\n",
    "        _df = _df[_df[\"region_code\"].str.startswith(\"ES\")][[\"region_code\", \"value\"]].copy()\n",
    "    \n",
    "    _df.rename(columns={\"value\": var_name}, inplace=True)\n",
    "\n",
    "    if X_vars_df is not None:\n",
    "        X_vars_df = pd.merge(X_vars_df, _df, on=\"region_code\", how=\"outer\")\n",
    "    else:\n",
    "        X_vars_df = _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vars_df.isna().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop 0  variance columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numerical_cols = X_vars_df.select_dtypes(include=['number'])\n",
    "variance = numerical_cols.var()\n",
    "\n",
    "# Identify columns with zero variance\n",
    "zero_variance_cols = variance[variance == 0].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: there are some variables in Germany that have 0 variance, but not in Spain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_variance_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### between paris of variables that are highly correlated, drop 1 variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_vars_corr = X_vars_df.copy()\n",
    "relevant_vars_corr.drop(columns=\"region_code\", inplace=True)\n",
    "\n",
    "corr_df = relevant_vars_corr.corr()\n",
    "\n",
    "for idx, row in corr_df.iterrows():\n",
    "    temp_dict = dict(row)\n",
    "    for key, value in temp_dict.items():\n",
    "        if (idx != key) & (value>=0.9):\n",
    "            print(f\"{idx} and {key} are highly correlated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: some variable pairs did not show high correlation in Germany, but do in Spain. This shows that relationships between variable pairs is different in the two countries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_to_drop = [\"discontinuous_urban_fabric_cover\", \"industrial_or_commercial_units_cover\", \n",
    "                \"employment_in_construction\", \"gross_domestic_product\", \"airports_cover\",\n",
    "                \"employment_in_manufacturing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vars_df.drop(columns=vars_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### missing data in Spain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_to_impute = [\"es_number_of_commerical_and_service_companies\",\n",
    "                    \"es_average_daily_traffic_light_duty_vehicles\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_to_impute in vars_to_impute:\n",
    "    y_var_df = pd.read_csv(\n",
    "        os.path.join(DATA_PATH, f\"{var_to_impute}.csv\")\n",
    "    )\n",
    "\n",
    "    y_var_df.rename(columns={\"value\": var_to_impute}, inplace=True)\n",
    "    y_var_df.drop(columns=\"year\", inplace=True)\n",
    "\n",
    "    final_df_with_reg_code = pd.merge(X_vars_df, y_var_df, on=\"region_code\", how=\"outer\")\n",
    "\n",
    "    final_df = final_df_with_reg_code.copy()\n",
    "    final_df.drop(columns=\"region_code\", inplace=True)\n",
    "\n",
    "    for corr_threshold in [0.1, 0.5]:\n",
    "        final_df = final_df.reindex(sorted(final_df.columns), axis=1)\n",
    "\n",
    "        correlations = final_df.corr()[[var_to_impute]].drop(var_to_impute)\n",
    "        correlations = correlations[(correlations[var_to_impute] <=-corr_threshold) | (correlations[var_to_impute] >=corr_threshold)]\n",
    "\n",
    "        correlations = correlations.transpose()\n",
    "\n",
    "        chosen_vars = list(correlations.columns)\n",
    "        chosen_vars.extend([var_to_impute])\n",
    "\n",
    "        save_df = final_df[chosen_vars].copy()\n",
    "\n",
    "        save_df.to_csv(os.path.join(\"..\", \"..\", \"data\", \n",
    "                                    \"missing_value_imputation\", \n",
    "                                    f\"{var_to_impute}_{corr_threshold}corr.csv\"), index=False)\n",
    "        \n",
    "        predictor_vars = list(save_df.columns)\n",
    "        predictor_vars.remove(var_to_impute)\n",
    "\n",
    "        with open(\n",
    "            os.path.join(\"..\", \"..\", \"data\", \"missing_value_imputation\", \n",
    "                         \"predictor_vars\", \n",
    "                         f\"{var_to_impute}_{corr_threshold}corr.json\"), \"w\"\n",
    "        ) as fp:\n",
    "            json.dump(list(predictor_vars), fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_display_names = var_df.set_index(\"var_name\")[\"display_name\"].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_var_display_names = {\"es_number_of_commerical_and_service_companies\": \"Number of commerical and service companies\",\n",
    "                    \"es_average_daily_traffic_light_duty_vehicles\": \"Average daily traffic - light duty vehicles\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = X_vars_df.copy()\n",
    "\n",
    "\n",
    "for var_to_impute in vars_to_impute:\n",
    "    y_var_df = pd.read_csv(\n",
    "        os.path.join(DATA_PATH, f\"{var_to_impute}.csv\")\n",
    "    )\n",
    "\n",
    "    y_var_df.rename(columns={\"value\": var_to_impute}, inplace=True)\n",
    "    y_var_df.drop(columns=\"year\", inplace=True)\n",
    "\n",
    "    corr_df = pd.merge(corr_df, y_var_df, on=\"region_code\", how=\"outer\")\n",
    "\n",
    "corr_df.drop(columns=\"region_code\", inplace=True)\n",
    "\n",
    "correlations = corr_df.corr()[vars_to_impute].drop(vars_to_impute)\n",
    "correlations = correlations.round(1)\n",
    "correlations = correlations.transpose()\n",
    "\n",
    "correlations.rename(columns=var_display_names, inplace=True)\n",
    "correlations.rename(index=y_var_display_names, inplace=True)\n",
    "\n",
    "low_correlations = correlations.loc[:, correlations.apply(lambda col: (col < 0.1).all(), axis=0)]\n",
    "\n",
    "high_correlations = correlations.loc[:, correlations.apply(lambda col: (col >= 0.1).any(), axis=0)]\n",
    "\n",
    "# Plotting the heatmap\n",
    "\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "gs = fig.add_gridspec(2, 1, wspace=0, hspace=2)\n",
    "\n",
    "# low correlations --------\n",
    "ax1 = plt.subplot(gs[:1, :])\n",
    "\n",
    "sns.heatmap(low_correlations.abs(), annot=True, cmap=cm.batlow_r, cbar=True,  cbar_kws={'shrink': 0.8, 'pad': 0.01}, vmin=0, vmax=1, ax=ax1)\n",
    "plt.xticks(rotation=45, ha=\"right\") \n",
    "plt.yticks(rotation=0) \n",
    "\n",
    "# high correlations --------\n",
    "\n",
    "ax2 = plt.subplot(gs[1:, :])\n",
    "\n",
    "sns.heatmap(high_correlations.abs(), annot=True, cmap=cm.batlow_r, cbar=True,  cbar_kws={'shrink': 0.8, 'pad': 0.01}, vmin=0, vmax=1, ax=ax2)\n",
    "plt.xticks(rotation=45, ha=\"right\") \n",
    "plt.yticks(rotation=0) \n",
    "\n",
    "plt.savefig(os.path.join(\"..\", \"..\", \"figures\", \"missing_value_imputation\", \n",
    "                            f\"es_nuts3_corr.png\"), \n",
    "                            bbox_inches='tight')  # Save the figure as a PNG file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
