{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from energyemissionsregio.config import DATA_PATH\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### potential predictor variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_df = pd.read_excel(\n",
    "    os.path.join(DATA_PATH, \"..\", \"..\", \"01_raw\", \"variables_with_details_and_tags.xlsx\"),\n",
    "    sheet_name=\"collected_variables_EU\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### potential predictor variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vars_lau = [\"population\", \n",
    "             \"area\", \n",
    "             'continuous_urban_fabric_cover',\n",
    "       'discontinuous_urban_fabric_cover',\n",
    "       'industrial_or_commercial_units_cover',\n",
    "       'port_areas_cover',\n",
    "       'airports_cover', 'mineral_extraction_sites_cover',\n",
    "       'dump_sites_cover', 'construction_sites_cover',\n",
    "       'green_urban_areas_cover', 'sport_and_leisure_facilities_cover',\n",
    "       'non_irrigated_arable_land_cover',\n",
    "       'permanently_irrigated_land_cover', 'rice_fields_cover',\n",
    "       'vineyards_cover', 'fruit_trees_and_berry_plantations_cover',\n",
    "       'olive_groves_cover', 'pastures_cover', 'permanent_crops_cover',\n",
    "       'complex_cultivation_patterns_cover',\n",
    "       'agriculture_with_natural_vegetation_cover',\n",
    "       'agro_forestry_areas_cover', 'broad_leaved_forest_cover',\n",
    "       'coniferous_forest_cover', 'mixed_forest_cover',\n",
    "       'natural_grasslands_cover', 'moors_and_heathland_cover',\n",
    "       'sclerophyllous_vegetation_cover',\n",
    "       'transitional_woodland_shrub_cover',\n",
    "       'beaches_dunes_and_sand_cover', 'bare_rocks_cover',\n",
    "       'sparsely_vegetated_areas_cover', 'burnt_areas_cover',\n",
    "       'glaciers_and_perpetual_snow_cover', 'inland_marshes_cover',\n",
    "       'peat_bogs_cover', 'salt_marshes_cover', 'salines_cover',\n",
    "       'intertidal_flats_cover', 'water_courses_cover',\n",
    "       'water_bodies_cover', 'coastal_lagoons_cover', 'estuaries_cover',\n",
    "       'sea_and_ocean_cover',\n",
    "       \"number_of_iron_and_steel_industries\",\n",
    "      \"number_of_cement_industries\",\n",
    "      \"number_of_refineries\",\n",
    "      \"number_of_paper_and_printing_industries\",\n",
    "      \"number_of_chemical_industries\",\n",
    "      \"number_of_glass_industries\",\n",
    "      \"number_of_non_ferrous_metals_industries\",\n",
    "      \"number_of_non_metallic_minerals_industries\",\n",
    "       \"railway_network\",\n",
    "       \"road_network\",\n",
    "       \"number_of_buildings\",\n",
    "       'average_air_pollution_due_to_pm25',\n",
    "       'average_air_pollution_due_to_no2',\n",
    "       'average_air_pollution_due_to_o3',\n",
    "       'average_air_pollution_due_to_pm10',\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vars_df = None\n",
    "\n",
    "for var_name in x_vars_lau:\n",
    "    \n",
    "    _df = pd.read_csv(\n",
    "        os.path.join(DATA_PATH, f\"{var_name}.csv\")\n",
    "    )\n",
    "    _df = _df[_df[\"region_code\"].str.startswith(\"ES\")][[\"region_code\", \"value\"]].copy()\n",
    "\n",
    "    _df = _df.fillna(0) # filling NAs for point vars. Non-point vars have no NAs in Spain\n",
    "\n",
    "    _df.rename(columns={\"value\": var_name}, inplace=True)\n",
    "\n",
    "    if X_vars_df is not None:\n",
    "        X_vars_df = pd.merge(X_vars_df, _df, on=\"region_code\", how=\"outer\")\n",
    "    else:\n",
    "        X_vars_df = _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vars_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop 0  variance columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numerical_cols = X_vars_df.select_dtypes(include=['number'])\n",
    "variance = numerical_cols.var()\n",
    "\n",
    "# Identify columns with zero variance\n",
    "zero_variance_cols = variance[variance == 0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_variance_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### between paris of variables that are highly correlated, drop 1 variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_vars_corr = X_vars_df.copy()\n",
    "relevant_vars_corr.drop(columns=\"region_code\", inplace=True)\n",
    "\n",
    "corr_df = relevant_vars_corr.corr()\n",
    "\n",
    "for idx, row in corr_df.iterrows():\n",
    "    temp_dict = dict(row)\n",
    "    for key, value in temp_dict.items():\n",
    "        if (idx != key) & (value>=0.9):\n",
    "            print(f\"{idx} and {key} are highly correlated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_to_impute = [\n",
    "\"es_utilized_agricultural_area\",\n",
    "\"es_number_of_dwellings\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_to_impute in vars_to_impute:\n",
    "    y_var_df = pd.read_csv(\n",
    "        os.path.join(DATA_PATH, f\"{var_to_impute}.csv\")\n",
    "    )\n",
    "    \n",
    "    y_var_df.rename(columns={\"value\": var_to_impute}, inplace=True)\n",
    "    y_var_df.drop(columns=\"year\", inplace=True)\n",
    "\n",
    "    final_df_with_reg_code = pd.merge(X_vars_df, y_var_df, on=\"region_code\", how=\"outer\")\n",
    "\n",
    "    final_df = final_df_with_reg_code.copy()\n",
    "    final_df.drop(columns=\"region_code\", inplace=True)\n",
    "    final_df.dropna(axis=0, inplace=True)\n",
    "\n",
    "    for corr_threshold in [0.1, 0.5]:\n",
    "        final_df = final_df.reindex(sorted(final_df.columns), axis=1)\n",
    "\n",
    "        correlations = final_df.corr()[[var_to_impute]].drop(var_to_impute)\n",
    "        correlations = correlations[(correlations[var_to_impute] <=-corr_threshold) | (correlations[var_to_impute] >=corr_threshold)]\n",
    "\n",
    "        correlations = correlations.transpose()\n",
    "\n",
    "        chosen_vars = list(correlations.columns)\n",
    "        chosen_vars.extend([var_to_impute])\n",
    "\n",
    "        save_df = final_df[chosen_vars].copy()\n",
    "\n",
    "        save_df.to_csv(os.path.join(\"..\", \"..\", \"data\", \n",
    "                                    \"missing_value_imputation\", \n",
    "                                    f\"{var_to_impute}_{corr_threshold}corr.csv\"), index=False)\n",
    "        \n",
    "        predictor_vars = list(save_df.columns)\n",
    "        predictor_vars.remove(var_to_impute)\n",
    "\n",
    "        with open(\n",
    "            os.path.join(\"..\", \"..\", \"data\", \"missing_value_imputation\", \n",
    "                         \"predictor_vars\", \n",
    "                         f\"{var_to_impute}_{corr_threshold}corr.json\"), \"w\"\n",
    "        ) as fp:\n",
    "            json.dump(list(predictor_vars), fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = X_vars_df.copy()\n",
    "for var_to_impute in vars_to_impute:\n",
    "    y_var_df = pd.read_csv(\n",
    "        os.path.join(DATA_PATH, f\"{var_to_impute}.csv\")\n",
    "    )\n",
    "\n",
    "    y_var_df.rename(columns={\"value\": var_to_impute}, inplace=True)\n",
    "    y_var_df.drop(columns=\"year\", inplace=True)\n",
    "\n",
    "    corr_df = pd.merge(corr_df, y_var_df, on=\"region_code\", how=\"outer\")\n",
    "\n",
    "corr_df.drop(columns=\"region_code\", inplace=True)\n",
    "\n",
    "correlations = corr_df.corr()[vars_to_impute].drop(vars_to_impute)\n",
    "correlations = correlations.round(1)\n",
    "correlations = correlations.transpose()\n",
    "\n",
    "correlations = correlations.loc[:, (correlations != 0).any()]\n",
    "\n",
    "low_correlations = correlations.loc[:, correlations.apply(lambda col: (col < 0.3).all(), axis=0)]\n",
    "\n",
    "high_correlations = correlations.loc[:, correlations.apply(lambda col: (col >= 0.3).any(), axis=0)]\n",
    "\n",
    "# Plotting the heatmap\n",
    "\n",
    "fig = plt.figure(figsize=(13, 6))\n",
    "gs = fig.add_gridspec(2, 1, wspace=0, hspace=2.5)\n",
    "\n",
    "# low correlations --------\n",
    "ax1 = plt.subplot(gs[:1, :])\n",
    "\n",
    "sns.heatmap(low_correlations.abs(), annot=True, cmap=cm.batlow_r, cbar=True,  cbar_kws={'shrink': 0.8, 'pad': 0.01}, vmin=0, vmax=1, ax=ax1)\n",
    "plt.xticks(rotation=45, ha=\"right\") \n",
    "plt.yticks(rotation=0) \n",
    "\n",
    "# high correaltions --------\n",
    "ax2 = plt.subplot(gs[1:, :])\n",
    "\n",
    "sns.heatmap(high_correlations.abs(), annot=True, cmap=cm.batlow_r, cbar=True,  cbar_kws={'shrink': 0.8, 'pad': 0.01}, vmin=0, vmax=1, ax=ax2)\n",
    "plt.xticks(rotation=45, ha=\"right\") \n",
    "plt.yticks(rotation=0) \n",
    "\n",
    "plt.savefig(os.path.join(\"..\", \"..\", \"figures\", \"missing_value_imputation\", \n",
    "                            f\"lau_es_corr.png\"), \n",
    "                            bbox_inches='tight')  # Save the figure as a PNG file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
